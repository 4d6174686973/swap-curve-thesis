{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Methods from de Prado\n",
    "from deprado import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Vector for Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the steepness data\n",
    "swap2s20s = pd.read_parquet('data/swap2s20s.parquet')\n",
    "\n",
    "# get y data and convert to decimal\n",
    "y_data = swap2s20s['MID_PRICE'].copy() / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Steepness\n",
    "plt.style.use(['science','ieee','no-latex'])\n",
    "plt.figure()\n",
    "plt.plot(y_data * 10000) # convert to bps\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Steepness [bps]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the Feature Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in feature space X\n",
    "X = pd.read_parquet(f'data/features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping y data to match X\n",
    "print(X.shape, y_data.shape)\n",
    "mask = y_data.index.intersection(X.index)\n",
    "y_data = y_data.loc[mask]\n",
    "X = X.loc[mask]\n",
    "print(X.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling\n",
    "- For labeling the data we use the input vector y_data that contains the 2s20s steepness\n",
    "- The function getEvents computes the times when barrier hits occur\n",
    "- The function getBins computes the actual labels based on when the barriers got hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgtval = 0.005  # since we use data in decimal format this number has to be multiplied by 10000 to obtain bps\n",
    "\n",
    " # the constant 50 bps target need to be set at every time step for the follwing functions\n",
    "trgt = pd.Series(data=np.full(y_data.shape[0], trgtval), index=y_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getEvents uses multiprocessing to speed up the process\n",
    "e = getEvents(y_data, trgt.index, ptSl=[1,1], trgt=trgt, minRet=0.00, numThreads=16, t1=False)\n",
    "bins = getBins(e, y_data, trgtval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to drop all nans\n",
    "y = bins['bin'].dropna()\n",
    "e = e.drop(e[e['t1'].isnull()].index)\n",
    "t1 = e['t1'].drop(e[e['t1'].isnull()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- not all original points were labeled because at the end of the data there may not occur barrier hits anymore\n",
    "- so we again need to equalize X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)\n",
    "mask = y.index.intersection(X.index)\n",
    "y = y.loc[mask]\n",
    "X = X.loc[mask]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end date of data that is used for the model\n",
    "print(X.index[0])\n",
    "print(X.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barrier Labeling Example plot\n",
    "\n",
    "y_copy = y_data.copy() * 10000\n",
    "date = '2009-05-07'\n",
    "event = e['t1'].loc[date]\n",
    "upper = y_copy.loc[date] + trgtval * 10000\n",
    "lower = y_copy.loc[date] - trgtval * 10000\n",
    "\n",
    "# Plot\n",
    "plt.style.use(['science','ieee','no-latex'])\n",
    "plt.figure()\n",
    "plt.plot(y_copy)\n",
    "plt.hlines(upper, pd.Timestamp(date), y_copy.index[-1], ls=':', color='r')\n",
    "plt.hlines(lower, pd.Timestamp(date), y_copy.index[-1], ls=':', color='r')\n",
    "plt.hlines(y_copy.loc[date], pd.Timestamp(date), y_copy.index[-1], ls=':', color='r')\n",
    "plt.vlines(pd.Timestamp(date), upper, lower, color='r')\n",
    "plt.annotate(\"Lower barrier is hit first\", xy=(pd.Timestamp(event), y_copy.loc[event]), xytext=(pd.Timestamp(event), lower - 200), arrowprops=dict(arrowstyle=\"->\",color='r'), color='r')\n",
    "plt.ylabel('Steepness [bps]')\n",
    "plt.xlabel('Date')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual labels\n",
    "\n",
    "y_copy = y_data.copy() * 10000\n",
    "plt.style.use(['science','ieee','no-latex'])\n",
    "plt.figure()\n",
    "plt.plot(y_copy)\n",
    "plt.scatter(bins['bin'].index[bins['bin'] == -1], y_copy.loc[bins.index].loc[bins['bin'] == -1], c='red', s=5)\n",
    "plt.scatter(bins['bin'].index[bins['bin'] == 1], y_copy.loc[bins.index].loc[bins['bin'] == 1], c='green', s=5)\n",
    "plt.legend(['Steepness', 'Label \"-1\"', 'Label \"1\"'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Steepness [bps]')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix\n",
    "- a clustering technique was used to resort the features so they form visible blocks in the matrix\n",
    "- the clustering algorithm was not included in the thesis because one could also sort the features manually in this case\n",
    "- so using the clustering algorithm is little bit pointless here but nonetheless comfortable to see visible blocks of high correlation without manually resorting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corr0 = np.corrcoef(X.T)\n",
    "corr0 = pd.DataFrame(corr0, columns=X.columns, index=X.columns)\n",
    "corr1, clstrs, silh = clusterKMeansTop(corr0=corr0, maxNumClusters=12, n_init=100)\n",
    "\n",
    "sns.heatmap(corr1, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation of Information Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr0var = varInfoMat(X, norm=True)\n",
    "corr1var, clstrsvar, silhvar = clusterKMeansTop(corr0=corr0var, maxNumClusters=12, n_init=100)\n",
    "\n",
    "sns.heatmap(corr1var, vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Average Uniqueness\n",
    "- this step also uses multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCoEvents = mpPandasObj(mpNumCoEvents, ('molecule', e.index), numThreads=12, closeIdx=y.index, t1=e['t1'])\n",
    "numCoEvents = numCoEvents.loc[~numCoEvents.index.duplicated(keep='last')]\n",
    "numCoEvents = numCoEvents.reindex(y.index).fillna(0)\n",
    "out = pd.DataFrame()\n",
    "out['tW'] = mpPandasObj(mpSampleTW, ('molecule',e.index), numThreads=12, t1=e['t1'], numCoEvents=numCoEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average uniqueness used for maxSamples in Classifiers\n",
    "avgU = out['tW'].mean()\n",
    "avgU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Features\n",
    "- here we drop all features that share high amount of information as motivated in the thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "colstodrop = ['M1 Money Supply', 'M2 Money Supply', 'M3 Money Supply', 'GDP', 'M2 Money Velocity']\n",
    "X.drop(colstodrop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot X with matplotlib\n",
    "plt.style.use(['science','ieee','no-latex'])\n",
    "cycler = plt.cycler(linestyle=['solid', 'solid', 'solid', 'solid', 'solid'],\n",
    "                color=['black', 'red', 'blue', 'green', 'grey'],\n",
    ")\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(cycler)\n",
    "plt.plot(X.loc['2016-01-01':])\n",
    "plt.legend(X.columns)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Feature Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Feature Importance\n",
    "- Finally we use the feature importane technique\n",
    "- Beware that the values here can slightly differ from that presented in the thesis\n",
    "- This is due to the randomized nature of training decision trees in the bagging classifier\n",
    "- However no large discrepancies should occur because the method is statistically robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up base classifier\n",
    "from sklearn.tree import DecisionTreeClassiﬁer\n",
    "from sklearn.ensemble import BaggingClassiﬁer\n",
    "\n",
    "clf=DecisionTreeClassifier(criterion='entropy',max_features=1, class_weight='balanced', min_weight_fraction_leaf=0)\n",
    "clf=BaggingClassifier(estimator=clf,n_estimators=1000,max_features=1.,max_samples=avgU,oob_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = featImpMDA(clf,X,y,10,t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['science','ieee','no-latex'])\n",
    "plt.figure()\n",
    "y_pos = np.arange(imp.shape[0])\n",
    "plt.scatter(y=y_pos, x=imp['mean'], marker='o', s=10)\n",
    "plt.errorbar(x=imp['mean'], y=y_pos, xerr=imp['std'], capsize=2, fmt='none')\n",
    "plt.yticks(y_pos, imp.index)\n",
    "plt.xlabel('Feature Importance Value F')\n",
    "plt.legend(['Mean', 'Standard Deviation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Base Classifier\n",
    "- Here we assess the performance of the base classifier by using 10-fold purged cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "n_splits = 10\n",
    "cvGen = PurgedKFold(n_splits=n_splits,t1=t1) # use purged cv\n",
    "scr0, scr1 = pd.Series(dtype=np.float64), pd.DataFrame(columns=clstrs.keys())  # make empty scorer\n",
    "cm = []\n",
    "for i,(train,test) in tqdm(enumerate(cvGen.split(X=X, y=y)), total=n_splits):\n",
    "    \n",
    "    # train and test by cv folds\n",
    "    X0, y0 = X.iloc[train,:], y.iloc[train]\n",
    "    X1, y1 = X.iloc[test,:], y.iloc[test]\n",
    "    \n",
    "    # fit classifier and compute score\n",
    "    fit = clf.fit(X=X0,y=y0)\n",
    "    prob = fit.predict_proba(X1)\n",
    "    \n",
    "    # compute accuracy\n",
    "    scr0.loc[i] = accuracy_score(y1, fit.predict(X1))\n",
    "\n",
    "    # compute confusion matrix\n",
    "    cm.append(confusion_matrix(y1, fit.predict(X1)))\n",
    "\n",
    "    # compute log loss\n",
    "    # scr0.loc[i] = -log_loss(y1, prob, labels=clf.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course also the accuracy differs but should also be in the range presented in the thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_mean = np.mean(cm, axis=0).astype(int)\n",
    "\n",
    "tp = cm_mean[1, 1]  # True Positives\n",
    "tn = cm_mean[0, 0]  # True Negatives\n",
    "total = tp + tn + cm_mean[0, 1] + cm_mean[1, 0]  # Total observations\n",
    "\n",
    "acc = (tp + tn) / total  # Accuracy\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm_mean, annot=True, \n",
    "            fmt=\"d\",\n",
    "            cbar=False,\n",
    "            xticklabels=[\"Negative\", \"Positive\"],\n",
    "            yticklabels=[\"Negative\", \"Positive\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
